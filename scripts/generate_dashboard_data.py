#!/usr/bin/env python3
"""
Manual Dashboard Data Generator
================================
Generates intelligence summaries and processes documents for dashboard widgets.

Usage:
    python scripts/generate_dashboard_data.py

What it does:
1. Generates daily intelligence for today (Activity Pulse, Intelligence Feed)
2. Generates weekly intelligence (Trends)
3. Checks if documents are processed in Neo4j (Entities, Deals)
4. Reports what data is available for each widget

This should populate all the dashboard widgets with real data.
"""
import os
import sys
import asyncio
from datetime import datetime, date, timedelta
from pathlib import Path
from decimal import Decimal

# Add parent directory to path to import app modules
sys.path.insert(0, str(Path(__file__).parent.parent))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Now import app modules
from supabase import create_client, Client
from app.services.intelligence.aggregator import (
    calculate_daily_metrics,
    calculate_weekly_trends,
    calculate_monthly_insights
)
from app.core.config import settings
from neo4j import AsyncGraphDatabase

import logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


async def main():
    """Main execution function."""
    logger.info("=" * 80)
    logger.info("Dashboard Data Generator - Manual Intelligence Generation")
    logger.info("=" * 80)

    # Initialize clients
    logger.info("ğŸ“¡ Initializing Supabase and Neo4j clients...")
    supabase: Client = create_client(
        settings.supabase_url,
        settings.supabase_anon_key
    )

    neo4j_driver = AsyncGraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_user, settings.neo4j_password)
    )

    logger.info("âœ… Clients initialized")

    # Step 1: Get tenant ID (first user in the system)
    logger.info("\n" + "=" * 80)
    logger.info("Step 1: Finding tenant ID...")
    logger.info("=" * 80)

    # Query for a document to get tenant_id
    result = supabase.table("documents").select("tenant_id").limit(1).execute()

    if not result.data:
        logger.error("âŒ No documents found! Sync some data first.")
        return

    tenant_id = result.data[0]["tenant_id"]
    logger.info(f"âœ… Found tenant ID: {tenant_id}")

    # Step 2: Check document counts
    logger.info("\n" + "=" * 80)
    logger.info("Step 2: Analyzing documents...")
    logger.info("=" * 80)

    # Get document stats
    docs = supabase.table("documents").select("*").eq("tenant_id", tenant_id).execute()
    total_docs = len(docs.data)

    # Group by source
    sources = {}
    for doc in docs.data:
        source = doc.get("source", "unknown")
        sources[source] = sources.get(source, 0) + 1

    logger.info(f"ğŸ“Š Total documents: {total_docs}")
    for source, count in sources.items():
        logger.info(f"   - {source}: {count} documents")

    # Step 3: Check Neo4j entities
    logger.info("\n" + "=" * 80)
    logger.info("Step 3: Checking Neo4j knowledge graph...")
    logger.info("=" * 80)

    async with neo4j_driver.session() as session:
        # Count all nodes (RAG entities don't have tenant_id, just check if graph is populated)
        result = await session.run("MATCH (n) RETURN count(n) as count")
        total_nodes = (await result.single())["count"]

        # Count PERSON nodes
        result = await session.run("MATCH (p:PERSON) RETURN count(p) as count")
        person_count = (await result.single())["count"]

        # Count COMPANY nodes
        result = await session.run("MATCH (c:COMPANY) RETURN count(c) as count")
        company_count = (await result.single())["count"]

        # Count PURCHASE_ORDER nodes
        result = await session.run("MATCH (po:PURCHASE_ORDER) RETURN count(po) as count")
        po_count = (await result.single())["count"]

    logger.info(f"ğŸ”¢ Total nodes in graph: {total_nodes}")
    logger.info(f"ğŸ‘¥ PERSON entities: {person_count}")
    logger.info(f"ğŸ¢ COMPANY entities: {company_count}")
    logger.info(f"ğŸ“ PURCHASE_ORDER entities: {po_count}")

    if person_count > 0 or company_count > 0:
        logger.info("âœ… Knowledge graph populated! Entities available via RAG search.")
    else:
        logger.warning("âš ï¸  No entities found in Neo4j!")
        logger.warning("   Documents exist but haven't been processed through RAG pipeline.")

    # Step 4: Generate daily intelligence
    logger.info("\n" + "=" * 80)
    logger.info("Step 4: Generating daily intelligence...")
    logger.info("=" * 80)

    today = date.today()
    logger.info(f"ğŸ“… Generating for date: {today}")

    try:
        daily_data = await calculate_daily_metrics(
            supabase=supabase,
            neo4j_driver=neo4j_driver,
            tenant_id=tenant_id,
            target_date=today
        )

        # Convert any date/decimal objects to JSON-serializable types
        def convert_dates(obj):
            if isinstance(obj, dict):
                return {k: convert_dates(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_dates(item) for item in obj]
            elif isinstance(obj, (date, datetime)):
                return obj.isoformat()
            elif isinstance(obj, Decimal):
                return float(obj)
            return obj

        daily_data_clean = convert_dates(daily_data)

        # Insert into daily_intelligence table
        supabase.table("daily_intelligence").upsert({
            "tenant_id": tenant_id,
            "date": str(today),
            **daily_data_clean
        }).execute()

        logger.info("âœ… Daily intelligence generated successfully!")
        logger.info(f"   Total documents: {daily_data.get('total_documents', 0)}")
        logger.info(f"   AI summary: {daily_data.get('ai_summary', 'N/A')[:100]}...")

    except Exception as e:
        logger.error(f"âŒ Failed to generate daily intelligence: {e}")
        import traceback
        traceback.print_exc()

    # Step 5: Generate weekly intelligence
    logger.info("\n" + "=" * 80)
    logger.info("Step 5: Generating weekly intelligence...")
    logger.info("=" * 80)

    # Get Monday of current week
    week_start = today - timedelta(days=today.weekday())
    logger.info(f"ğŸ“… Generating for week starting: {week_start}")

    try:
        weekly_data = await calculate_weekly_trends(
            supabase=supabase,
            neo4j_driver=neo4j_driver,
            tenant_id=tenant_id,
            week_start=week_start
        )

        # Convert any date/decimal objects to JSON-serializable types
        def convert_dates(obj):
            if isinstance(obj, dict):
                return {k: convert_dates(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_dates(item) for item in obj]
            elif isinstance(obj, (date, datetime)):
                return obj.isoformat()
            elif isinstance(obj, Decimal):
                return float(obj)
            return obj

        weekly_data_clean = convert_dates(weekly_data)

        # Insert into weekly_intelligence table
        supabase.table("weekly_intelligence").upsert({
            "tenant_id": tenant_id,
            "week_start": str(week_start),
            "week_end": str(week_start + timedelta(days=6)),
            **weekly_data_clean
        }).execute()

        logger.info("âœ… Weekly intelligence generated successfully!")
        logger.info(f"   Total documents: {weekly_data.get('total_documents', 0)}")
        logger.info(f"   WoW change: {weekly_data.get('wow_change_percent', 0)}%")

    except Exception as e:
        logger.error(f"âŒ Failed to generate weekly intelligence: {e}")
        import traceback
        traceback.print_exc()

    # Step 6: Generate monthly intelligence
    logger.info("\n" + "=" * 80)
    logger.info("Step 6: Generating monthly intelligence...")
    logger.info("=" * 80)

    # First day of current month
    month_start = today.replace(day=1)
    logger.info(f"ğŸ“… Generating for month: {month_start}")

    monthly_data = None
    try:
        monthly_data = await calculate_monthly_insights(
            supabase=supabase,
            neo4j_driver=neo4j_driver,
            tenant_id=tenant_id,
            month=month_start
        )

        # Convert any date objects to strings for JSON serialization
        def convert_dates(obj):
            if isinstance(obj, dict):
                return {k: convert_dates(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_dates(item) for item in obj]
            elif isinstance(obj, (date, datetime)):
                return obj.isoformat()
            return obj

        monthly_data_clean = convert_dates(monthly_data)

        # Insert into monthly_intelligence table
        supabase.table("monthly_intelligence").upsert({
            "tenant_id": tenant_id,
            "month": str(month_start),
            **monthly_data_clean
        }).execute()

        logger.info("âœ… Monthly intelligence generated successfully!")
        logger.info(f"   Total documents: {monthly_data.get('total_documents', 0)}")
        logger.info(f"   Total revenue: ${monthly_data.get('total_revenue', 0):,.2f}")

    except Exception as e:
        logger.error(f"âŒ Failed to generate monthly intelligence: {e}")
        logger.warning("   This may be due to missing table columns (migration not run)")
        logger.warning("   Monthly intelligence is optional - daily/weekly data should work fine")

    # Step 7: Summary report
    logger.info("\n" + "=" * 80)
    logger.info("Dashboard Widget Status Report")
    logger.info("=" * 80)

    widgets = [
        ("Activity Pulse", "daily_intelligence table populated" if daily_data else "âŒ Failed"),
        ("Intelligence Feed", "daily + weekly intelligence generated" if daily_data and weekly_data else "âŒ Failed"),
        ("Trending Entities", f"âœ… {person_count} people, {company_count} companies" if person_count > 0 else "âŒ No entities in Neo4j"),
        ("Deal Momentum", f"âœ… {po_count} purchase orders tracked" if po_count > 0 else "âŒ No deals in Neo4j"),
        ("Sentiment Alerts", f"âœ… {total_docs} documents available for analysis"),
        ("Communication Patterns", f"âœ… {sources.get('gmail', 0) + sources.get('outlook', 0)} emails available"),
    ]

    logger.info("\nğŸ“Š Widget Readiness:")
    for widget_name, status in widgets:
        logger.info(f"   {widget_name:25s} â†’ {status}")

    # Close connections
    await neo4j_driver.close()

    logger.info("\n" + "=" * 80)
    logger.info("âœ… Dashboard data generation complete!")
    logger.info("=" * 80)
    logger.info("\nğŸ¯ Next steps:")
    logger.info("   1. Refresh your dashboard in the browser")
    logger.info("   2. Check that Activity Pulse and Intelligence Feed now show data")
    logger.info("   3. If Trending Entities still empty, documents need RAG processing")
    logger.info("   4. Run this script again tomorrow to generate more historical data")
    logger.info("")


if __name__ == "__main__":
    asyncio.run(main())
